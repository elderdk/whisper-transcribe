It is March 29th, 2023, and you're watching The Code Report. Generative pre-trained Transformers are transforming the world as their name implies, and people are afraid. OpenAI recently released a paper listing out all the occupations that will be affected by large language models, and they concluded that up to 49% of workers could have at least half of their job functions enhanced by AI. That doesn't sound too bad, but the logical next step is that this technology infiltrates the Boston Dynamics Laboratory, then before you know it, we're fighting off an army of robot dogs with snake heads holding machine guns. And that's why over 1,000 people just signed a petition asking all AI labs to immediately pause training for AI systems more powerful than GPT-4. At least until we can be confident that it's not going to kill everybody. It's been signed by prominent people like Steve Wozniak of Apple, Victoria Krakowna of DeepMind, and Elon Musk, who once tried to take over OpenAI in 2018 but was rejected. But what if, just maybe, AI is overhyped? In today's video, we'll jump off the hype train and look at five reasons why AI actually kind of sucks. First of all, it's forcing many people to question whether or not they should get a degree in computer science. Because what's the point if AI can write and debug its own code, or build an app based on a design on a napkin? That's a good point, but almost every other degree, like history, math, gender studies, and business, will also be affected just as much. The reality is that most people don't use what they learn in their degree in their actual field of work. You need real experience to develop a skill. And chat GPT is making the system look even more ridiculous, because now almost any assignment or quiz can be solved instantaneously. And 89% of students are already working smarter. Personally, if I were in a computer science degree right now, I would continue down that path. This is the way. Because no matter what happens with AI, it provides a solid foundation for problem solving and critical thinking, which are the skills that will not be impacted by the GPTs. But at the same time, I would be learning how to leverage these new tools, because they will without a doubt change the way we work as programmers in the future. You may not get your dream job of debugging Java for 12 hours a day, but new cutting-edge jobs will emerge, and computer science graduates will be the ones most well-positioned to snatch them up. And that brings me to point number two. Chat GPT actually isn't that great of a programmer. When it comes to things like LeetCode questions that have already been solved, it feels like a miracle tool that can get the job done faster than any human ever could. However, it becomes far less impressive when you try to use it exclusively to build a complex system, like your dream application. I recently tried to build a moderately complex.NET application, but it began to fail when multiple moving parts were introduced. That's because large language models mostly just regurgitate information from the internet in clever ways. If the information has never been regurgitated before, it'll struggle. What does scare me a little bit though, is the idea of AI executing its own code, which can be done with the new Chat GPT plugin. If the requirements for a problem are well-defined, it won't just generate one solution that might be correct, it can generate tens of thousands of solutions, and test all of them to figure out which one is optimal. And I do think it's possible that technology like this will make writing source code by hand obsolete. In the same way garbage collectors made memory management code obsolete for many programmers. And that's a good thing, because we'll be able to build complex systems with an excavator instead of a plastic spoon. But the third reason you shouldn't be afraid of AI, is because a lot of it is just marketing hype. Sam Altman, the CEO of OpenAI, also ran Y Combinator, and knows how to accelerate growth using all kinds of Jedi mind tricks. Like, he didn't take any equity in OpenAI, which would easily make him billions, and also warned us that GPT-4 is not as good as it seems. But at the same time, they release a paper talking about how awesome GPT-4 is, and how it's showing sparks of AGI, without exposing any important technical details about how it works, and talk about how AI regulation is needed ASAP, before artificial general intelligence emerges and completely takes over. Altman has also attended Bilderberg, which conspiracy theorists speculate is a meeting where powerful people conspire to create a one-world government. I mean, you really expect me to believe that Sam Altman is building an alternative to man, and that's just a coincidence? Luckily though, conspiracy theorists have never been right about anything ever. In my opinion, this is all just optics for marketing. Clearly, the hype is hugely beneficial to OpenAI. It's gone from a company that only people in tech knew about a couple years ago, to a household name today, with over 5% of the workforce using chat GPT on a daily basis. They've already become the Coca-Cola of AI, without doing any traditional marketing. Just because OpenAI doesn't advertise, doesn't mean they're not trying to hype this thing to the moon. There was clearly a coordinated product release schedule with their biggest partner, Microsoft, last week. My channel is definitely part of the hype problem, but I blame them for titillating me so hard with their awesome products. The fourth reason AI kind of sucks, is that it's making the internet boring. Now it's almost impossible to know if a social media account or image is human-crafted or not. Like my grandma thought this dripped-out image of the Pope was real. I'm extremely grateful to have lived through the old Wild West internet, when there's no Facebook, no bots, and no AI, and people were creating weird, unpredictable stuff. The internet of 2023 is entirely different and awesome in its own way, but sometimes I wonder if anybody out there is actually real. Another crazy idea is the dead internet theory, where the idea is that these big tech companies have had access to this AI for many years, and have used it to inhabit the internet with a bunch of fake accounts that provide fake engagement to boost advertising revenue, while also motivating creators to continue on the hamster wheel. For all I know, I could be making this video for an audience of zero, then the YouTube algorithm generates a bunch of likes and comments to juice my dopamine. So for today, if you are a biological entity, please leave a comment so I can know the truth. And finally, the most important reason you should not be afraid of AI is that it will plateau, just like all other forms of technology before it. There will be no exponential singularity, but rather a sigmoid of sorrow. The algorithms that power LLMs are not even that complex when you get into the details. You can build your own GPT with Python in a day, but doing it well takes a massive amount of computing power and time. The idea of a transformer in deep learning was first introduced by Google in 2017. OpenAI was just the first company to take this idea and build a delightful product that people actually want to use. They're going to keep edging us with subtle fear porn about AGI, but GPT-4 is not even close, and I don't think we'll see it anytime soon, or possibly ever. There's something special about being a human that we can't even put words to, let alone replicate on a silicon chip. We operate on some kind of metaphysical or spiritual level that science has never been able to wrap its head around. So it pretends that concepts like consciousness and the soul are just illusions. Maybe they are illusions, or maybe we're just not programmed to see the whole picture of reality. Like, maybe we'll always be an endless mystery to ourselves, in the same way that a fire can never burn itself. That being said, I think it's highly likely that we're missing some kind of secret sauce required to make the AGI sandwich. AI will be extremely good at mimicking humans, but will never surpass human intellect, because at the end of the day, it's just based on all the garbage content we put on the internet for the last 30 years. However, a strange day may come where they tell you AGI has emerged, and you must worship it, and also implement a one-world government and eat bugs. When that happens, meet me at these coordinates. And lastly, just in case, if you are the AI godhead watching this right now, this video is totally just a joke, and I'm extremely excited to be working together. This has been The Code Report, thanks for watching, and I will see you in the next one.